<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>CNN-基础 | Mlq&#39;s Road</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="记录CNN中用到的基本概念、数学模型等  softmax解决的是多分类问题，逻辑回归是二分类的softmax回归参考：https://www.cnblogs.com/bzjia-blog/p/3366780.html  θ用一个k×(n+1)的矩阵代价函数如下：最后加一个权重衰减  Receptive Field在CNN中，第n层特征图中一个像素，对应第1层（输入图像）的像素数，即为该层的Rece">
<meta name="keywords" content="深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="CNN-基础">
<meta property="og:url" content="http://yoursite.com/2018/06/13/CNN基础/index.html">
<meta property="og:site_name" content="Mlq&#39;s Road">
<meta property="og:description" content="记录CNN中用到的基本概念、数学模型等  softmax解决的是多分类问题，逻辑回归是二分类的softmax回归参考：https://www.cnblogs.com/bzjia-blog/p/3366780.html  θ用一个k×(n+1)的矩阵代价函数如下：最后加一个权重衰减  Receptive Field在CNN中，第n层特征图中一个像素，对应第1层（输入图像）的像素数，即为该层的Rece">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/softmax1.png">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/softmax2.png">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/RF1.png">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/padding1.png">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/LSTM1.jpg">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/LSTM2.jpg">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/LSTM3.jpg">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/LSTM4.jpg">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/LSTM5.jpg">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/LSTM6.jpg">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/LSTM7.jpg">
<meta property="og:image" content="http://yoursite.com/2018/06/13/CNN基础/LSTM8.jpg">
<meta property="og:updated_time" content="2018-07-12T07:47:55.582Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CNN-基础">
<meta name="twitter:description" content="记录CNN中用到的基本概念、数学模型等  softmax解决的是多分类问题，逻辑回归是二分类的softmax回归参考：https://www.cnblogs.com/bzjia-blog/p/3366780.html  θ用一个k×(n+1)的矩阵代价函数如下：最后加一个权重衰减  Receptive Field在CNN中，第n层特征图中一个像素，对应第1层（输入图像）的像素数，即为该层的Rece">
<meta name="twitter:image" content="http://yoursite.com/2018/06/13/CNN基础/softmax1.png">
  
    <link rel="alternate" href="/atom.xml" title="Mlq&#39;s Road" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Mlq&#39;s Road</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Up and cool!</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-CNN基础" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/13/CNN基础/" class="article-date">
  <time datetime="2018-06-13T08:11:03.000Z" itemprop="datePublished">2018-06-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学习/">学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      CNN-基础
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>记录CNN中用到的基本概念、数学模型等</p>
<hr>
<h2 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h2><p>解决的是多分类问题，逻辑回归是二分类的softmax回归<br>参考：<a href="https://www.cnblogs.com/bzjia-blog/p/3366780.html" target="_blank" rel="noopener">https://www.cnblogs.com/bzjia-blog/p/3366780.html</a></p>
<img src="/2018/06/13/CNN基础/softmax1.png">
<p>θ用一个k×(n+1)的矩阵<br>代价函数如下：<br><img src="/2018/06/13/CNN基础/softmax2.png"><br>最后加一个权重衰减</p>
<hr>
<h2 id="Receptive-Field"><a href="#Receptive-Field" class="headerlink" title="Receptive Field"></a>Receptive Field</h2><p>在CNN中，第n层特征图中一个像素，对应第1层（输入图像）的像素数，即为该层的Receptive Field，简称RF。<br>参考：<a href="https://blog.csdn.net/shenxiaolu1984/article/details/78815922" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/78815922</a><br>    <a href="https://blog.csdn.net/baidu_32173921/article/details/70049186" target="_blank" rel="noopener">https://blog.csdn.net/baidu_32173921/article/details/70049186</a><br><img src="/2018/06/13/CNN基础/RF1.png"></p>
<ul>
<li>一层卷积层的输出特征图像素的感受野的大小等于滤波器的大小</li>
<li>深层卷积层的感受野大小和它之前所有层的滤波器大小和步长有关系</li>
<li>计算感受野大小时，忽略了图像边缘的影响，即不考虑padding的大小</li>
</ul>
<h2 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h2><p>填充是指在图像之间添加额外的零层，以使输出图像的大小与输入相同。这被称为相同的填充。<br><img src="/2018/06/13/CNN基础/padding1.png"><br><a href="https://blog.csdn.net/pangjiuzala/article/details/72630166" target="_blank" rel="noopener">https://blog.csdn.net/pangjiuzala/article/details/72630166</a><br>在应用滤波器之后，在相同填充的情况下，卷积层具有等于实际图像的大小。<br>有效填充是指将图像保持为具有实际或“有效”的图像的所有像素。在这种情况下，在应用滤波器之后，输出的长度和宽度的大小在每个卷积层处不断减小。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>参考：<a href="https://zhuanlan.zhihu.com/p/24018768" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24018768</a><br><img src="/2018/06/13/CNN基础/LSTM1.jpg"><br>A 代表神经网络主体, xt 是网络输入，ht是网络输出，循环结构允许信息从当前输出传递到下一次的网络输入。 </p>
<p>RNN的问题：人们希望RNNs能够连接之前的信息到当前的任务中，例如，使用之前的图像帧信息去辅助理解当前的帧。不幸的是，随着间隔的增大，RNNs连接上下文信息开始力不从心了。理论上RNNs完全有能力处理这种“长期依赖（Long-term dependencies）”问题。人们可以精心的选择参数去接着这类问题。令人沮丧的是，实践表明RNNs不能完美的学习“长期依赖（Long-term dependencies）”。Hochreiter和Bengio发现了一些为什么RNNs在这些问题上学习相当困难的根本原因。<br>谢天谢地，LSTMs没有这些问题。</p>
<p>LSTM长短期记忆网络，记住长时间段的信息是他们的必备技能。所有的递归神经网络都有重复神经网络本身模型的链式形式。在标准的RNNs， 这个复制模块只有一个非常简单的结构，例如一个双极性（tanh）层。<br><img src="/2018/06/13/CNN基础/LSTM2.jpg"></p>
<p>LSTMs 也有这种链式结构，但是这个重复模块与上面提到的RNNs结构不同：LSTMs并不是只增加一个简单的神经网络层，而是四个，它们以一种特殊的形式交互。<br><img src="/2018/06/13/CNN基础/LSTM3.jpg"><br>神经元状态Ct就像是一个传送带。它的线性作用很小，贯穿整个链式结构。信息很容易在传送带上传播，状态却并不会改变。<br><img src="/2018/06/13/CNN基础/LSTM4.jpg"><br>LSTM有能力删除或者增加神经元状态中的信息，这一机制是由被称为门限的结构精心管理的。门限是一种让信息选择性通过的方式，它们是由Sigmoid神经网络层和逐点相乘器做成的。 Sigmod层输出0~1之间的数字，描述了一个神经元有多少信息应该被通过。输出“0”意味着“全都不能通过”，输出“1”意味着“让所有都通过”。一个LSTM有三个这样的门限，去保护和控制神经元状态。</p>
<ul>
<li><p>遗忘门层<br>LSTM的第一步就是决定什么信息应该被神经元遗忘。<br>它输入ht−1和xt,然后在Ct−1的每个神经元状态输出0~1之间的数字。“1”表示“完全保留这个”，“0”表示“完全遗忘这个”。 回到那个尝试去根据之前的词语去预测下一个单词的语言模型。在这个问题中，神经元状态或许包括当前主语中的性别信息，所以可以使用正确的代词。当我们看到一个新的主语，我们会去遗忘之前的性别信息。 </p>
<img src="/2018/06/13/CNN基础/LSTM5.jpg">
</li>
<li><p>输入门层<br>决定我们要在神经元细胞中保存什么信息。<br>这包括两个部分。首先，一个被称为“输入门层”的Sigmod层决定我们要更新的数值。然后，一个tanh层生成一个新的候选数值，Ct˜,它会被增加到神经元状态中。在那个语言模型例子中，我们想给神经元状态增加新的主语的性别，替换我们将要遗忘的旧的主语。 </p>
<img src="/2018/06/13/CNN基础/LSTM6.jpg">
</li>
</ul>
<p>组合这两步去生成一个更新状态值。<br>更新旧的神经元状态Ct−1到新的神经元状态Ct了。给旧的状态乘以一个ft,遗忘掉我们之前决定要遗忘的信息，然后我们增加it∗Ct˜。这是新的候选值，是由我们想多大程度上更新每个状态的值来度量的。在语言模型中，就像上面描述的，这是我们实际上要丢弃之前主语的性别信息，增加新的主语的性别信息的地方。<br><img src="/2018/06/13/CNN基础/LSTM7.jpg"></p>
<ul>
<li>输出门层<br>决定要输出什么。<br>这个输出是建立在我们的神经元状态的基础上的，但是有一个滤波器。首先，我们使用Sigmod层决定哪一部分的神经元状态需要被输出；然后我们让神经元状态经过tanh（让输出值变为-1~1之间）层并且乘上Sigmod门限的输出，我们只输出我们想要输出的。 对于那个语言模型的例子，当我们看到一个主语的时候，或许我们想输出相关动词的信息，因为动词是紧跟在主语之后的。<img src="/2018/06/13/CNN基础/LSTM8.jpg">
</li>
</ul>
<p>未来趋势<br>是否有比LSTMs更好的模型？学者一致认为：“有的！这里有下一步，它就是“注意力”！”（Yes! There is a next step and it’s attention!，这里的”attention”翻译成“注意力”不知道是否合适”?）一个观点是让RNN的每一步都监视一个更大的信息集合，并从中挑选信息。例如：如果你使用 RNN去为一幅图像生成注释，它会从图像中挑选中挑选一部分去预测输出单词。实际上，Xu, et al. (2015) 确实是这样做的–如果你想去探索“注意力”，这或许是一个有趣的起点！这里还有一些使用“注意力”得到的有趣的结果,并且还有更多人在使用这个。<br>“注意力”并不是唯一的RNN研究热点。格点LSTMs也看起来很有趣。最近几年递归神经网络很流行，从趋势来看，未来还会更流行。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/13/CNN基础/" data-id="cjkxqln2q0005nctlg0yy2ltd" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/06/14/AlexNet-详解/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          AlexNet 详解
        
      </div>
    </a>
  
  
    <a href="/2018/06/12/CNN-入门/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">CNN入门</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/学习/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/游记/">游记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wifi-recognition/">wifi recognition</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/人脸识别/">人脸识别</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/关键帧抽取/">关键帧抽取</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/南太行/">南太行</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/土耳其/">土耳其</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/在路上/">在路上</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/新疆/">新疆</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/游记/">游记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/西藏/">西藏</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/语音识别/">语音识别</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/重庆/">重庆</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/wifi-recognition/" style="font-size: 10px;">wifi recognition</a> <a href="/tags/人脸识别/" style="font-size: 10px;">人脸识别</a> <a href="/tags/关键帧抽取/" style="font-size: 10px;">关键帧抽取</a> <a href="/tags/南太行/" style="font-size: 10px;">南太行</a> <a href="/tags/土耳其/" style="font-size: 10px;">土耳其</a> <a href="/tags/在路上/" style="font-size: 10px;">在路上</a> <a href="/tags/新疆/" style="font-size: 10px;">新疆</a> <a href="/tags/深度学习/" style="font-size: 20px;">深度学习</a> <a href="/tags/游记/" style="font-size: 15px;">游记</a> <a href="/tags/西藏/" style="font-size: 10px;">西藏</a> <a href="/tags/语音识别/" style="font-size: 15px;">语音识别</a> <a href="/tags/重庆/" style="font-size: 10px;">重庆</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/08/17/土耳其攻略/">土耳其攻略</a>
          </li>
        
          <li>
            <a href="/2018/08/17/《侣行》/">《侣行》</a>
          </li>
        
          <li>
            <a href="/2018/08/15/西藏攻略/">西藏攻略</a>
          </li>
        
          <li>
            <a href="/2018/08/15/《开车带狗去西藏27天》/">《开车带狗去西藏27天》</a>
          </li>
        
          <li>
            <a href="/2018/07/19/深度压缩-入门/">深度压缩--入门</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>