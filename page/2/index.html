<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Mlq&#39;s Road</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Mlq&#39;s Road">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Mlq&#39;s Road">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mlq&#39;s Road">
  
    <link rel="alternate" href="/atom.xml" title="Mlq&#39;s Road" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Mlq&#39;s Road</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Up and cool!</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-语音识别" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/27/语音识别/" class="article-date">
  <time datetime="2018-06-27T04:16:14.000Z" itemprop="datePublished">2018-06-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学习/">学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/27/语音识别/">语音识别调研</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>语音识别技术难点</li>
<li>语音识别方法</li>
<li>现有技术<h2 id="技术原理"><a href="#技术原理" class="headerlink" title="技术原理"></a>技术原理</h2>参考 <a href="https://www.zhihu.com/question/20398418/answer/167412177" target="_blank" rel="noopener">https://www.zhihu.com/question/20398418/answer/167412177</a><h3 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h3><img src="/2018/06/27/语音识别/1.png">
</li>
</ul>
<p>W——文字序列，Y——语音输入。<br>公式1表示语音识别的目标是在给定语音输入的情况下，找到可能性最大的文字序列。<br>根据Baye’ Rule，可以得到公式2，其中分母表示出现这条语音的概率，它相比于求解的文字序列没有参数关系，可以在求解时忽略，进而得到公式3。<br>公式3中第一部分表示给定一个文字序列出现这条音频的概率，它就是语音识别中的声学模型；第二部分表示出现这个文字序列的概率，它就是语音识别中的语言模型。</p>
<h3 id="声学模型"><a href="#声学模型" class="headerlink" title="声学模型"></a>声学模型</h3><p>声学模型可以理解为是对发声的建模，它能够把语音输入转换成声学表示的输出，更准确的说是给出语音属于某个声学符号的概率。</p>
<h4 id="传统模型"><a href="#传统模型" class="headerlink" title="传统模型"></a>传统模型</h4><p>在英文中这个声学符号可以是音节（syllable）或者更小的颗粒度音素（phoneme）；在中文中这个声学符号可以是声韵母或者是颗粒度同英文一样小的音素。</p>
<img src="/2018/06/27/语音识别/3.png">
<p>Q——发音单位的序列。<br>从公式中可以看到，声学模型最终转换成了一个语音到发音序列的模型和一个发音序列到输出文字序列的字典。<br>这里的发音序列通常是音素，到此为止声学模型是从语音到音素状态的一个描述。<br>为了对不同上下文的音素加以区分，通常使用上下文相关的“三音子”作为建模单元。</p>
<p>声学建模的颗粒度可以继续分解为更小的状态（state）。通常一个三音子对应有3个状态（静音通常是5个状态），那么声学建模的总数就是3×Q^3+5这么多。为了压缩建模单元数量，状态绑定的技术被大量使用。</p>
<p>声学模型是一个描述语音和状态之间转换的模型。引入HMM假设：状态隐变量，语音是观测值，状态之间的跳转符合马尔科夫假设。那么声学模型可以继续表示为红框部分，a表示转移概率，b表示发射概率。</p>
<p>观测概率通常用GMM或是DNN来描述。这就是CD-GMM-HMM架构和CD-DNN-HMM架构的语音识别声学模型。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/27/语音识别/" data-id="cjkxqln3j0017nctl2yyk5sah" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/语音识别/">语音识别</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-重庆重庆" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/22/重庆重庆/" class="article-date">
  <time datetime="2018-06-22T15:48:25.000Z" itemprop="datePublished">2018-06-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/游记/">游记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/22/重庆重庆/">重庆重庆</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>地形地势给了重庆天生之美，美食美女美景。抖音为重庆又添了一层热度，空气里弥漫着浓浓的火锅味和抖音神曲，享受城市的便利却又不舍得它沾染太多商业气息。重庆，本应是慢节奏的，却不得不加快脚步跟随人潮，淹没在拥挤的游客大潮中。</p>
<h2 id="第1天"><a href="#第1天" class="headerlink" title="第1天"></a>第1天</h2><p>凌晨抵达重庆，重庆机场比北京还有国际范儿，打车到附近青旅，出租车司机脾气很大，弯路上狂飙。很近的路拐了很久才到旅店，当时就该意识到一切地图在重庆都失灵了，这种认知在接后两天如影随形。清晨在机场附近看到一群传说中的棒棒工，大概在等着揽活，一天走下来才懂得多么不容易。<br><img src="/2018/06/22/重庆重庆/d1_1.png"><br>傍晚去第二家青旅，看了地图距离不远，而且有条路比导航路线近，尝试走回去，走到附近才发现不通。打车到旅店楼下，师傅问要不要开车绕上去，客气了一下就地下车，两分钟后肠子都悔青了，世界上最遥远的距离莫过于目的地就在你上方，你却找不到直升的翅膀。明明是紧挨着的两面，我们在地下好几层，门却在地上八层的另一边，舍不得再交智商税，咬牙开始绕1.5公里的距离，事实再次打脸，在重庆永远不要相信所谓的距离，爬呀爬呀爬半个小时才终于从楼的一面走到另一面，内心极其崩溃，然而更打脸的是明明旁边洪崖洞就有直升梯，直到离开才发现。</p>
<img src="/2018/06/22/重庆重庆/d2_1.png">
<p>蒙奇奇青旅是住过青旅中印象最深刻的，青旅该有的难有的在这里都能找到，实在不想把它简单定义成青旅而已。处在洪崖洞旁边，八层之上俯瞰洪崖洞夜景与嘉陵江江景，入睡前睁眼后坐在阳台沙发上看着窗外，可以发呆很久甚至想一直坐下去。洗衣房还有几个月大的小萌猫，粘人又可爱，分分钟融化你心，第一晚遇到一群当天拼成团的小伙伴，嬉闹到一两点。</p>
<h2 id="第2天"><a href="#第2天" class="headerlink" title="第2天"></a>第2天</h2><p>第一天的火锅太厉害，夜里不停的上厕所，一共四个小时的睡眠却丝毫没有影响第二天的精神。跟团向着武隆景区出发，整个武隆都是政府特意开发的景区，包括村子里新建的高楼，景区就餐区等，有的在建有的建完还空着，不知该为当地经济发展高兴，还是为这商业化遗憾。</p>
<img src="/2018/06/22/重庆重庆/d2_2.png">
<p>第一站点是玻璃站台，俯瞰天坑，人很多但是不妨碍初见美景的好心情。武隆景区的开发源自满城尽带黄金甲中张艺谋导演的取景，说来奇怪，整个电影我还真就只记得瓦屋遇袭的场景。多年来风雨侵蚀形成武隆三桥两坑的构造。这种由时间打磨出的奇观令人惊叹。下了很长一段台阶才下到坑底，难以想象当年未开发时取景拍摄多么不易。</p>
<img src="/2018/06/22/重庆重庆/d2_3.png">
<p>看山需要想象，可惜行程紧凑，大多数奇妙的形状都是导游引导出来，虽然没有遗漏但总有些不甘，路上的游客也是惊叹一声便拿出相机猛拍，很少有静静的欣赏用心感受的游者。</p>
<img src="/2018/06/22/重庆重庆/d2_4.png">
<p>南方的山格外潮湿，甚至有泉水从石壁中流淌而出，溅到空中晶莹剔透，弹到脸上有清凉之感。长时间的水蚀作用让石壁呈现出奇妙的纹理，也催生出许多绿色的苔藓加以点缀。</p>
<img src="/2018/06/22/重庆重庆/d2_5.png">
<p>湿润气候最不缺的就是植物，它们从各种意想不到的地方冒出来，给整个坑底披上绿衣，360°满是风景。放一张无聊的妹妹，景区为游客拍免费留念照时，寂寞的小身影。</p>
<img src="/2018/06/22/重庆重庆/d2_6.png">
<p>坐观光梯下到地缝，继续一路往下走，在我眼里重庆的一天要么是一直往上走，要么是一直往下走，无限神奇。地缝这边都是贴着石壁的栈道，慢慢走别有一番滋味，印象比较深的是有个山洞，很像笑傲江湖中练功之所，最妙的是这里有条地下暗河，在很深的地方只能听到水声，看到水流从洞中倾泻而下。在后面形成瀑布，视觉感很强烈，从瀑布底下穿过才真正有了水帘洞的感觉。整个景区信号非常好，在地缝里收到航班被取消的短信，很顺利就改了航班，改早了航班心情更舒畅。</p>
<img src="/2018/06/22/重庆重庆/d2_7.png">
<p>晚上回到洪崖洞逛了一下，确实有千与千寻的画面感，房子上建房子，从解放碑过来发现解放碑的位置放在这里是10层，而地下还有一层通行区域。依山而建依势而落，总能让你惊叹。</p>
<h2 id="第3天"><a href="#第3天" class="headerlink" title="第3天"></a>第3天</h2><p>之所以这里称为第三天，因为自打进了蒙奇奇就没两点前上过床，太多有意思的人，有意思的话题，再加上老板不停的劝酒，这一夜把酒言欢闹到了晚上四点。从重庆四川对骂比拼、到经历分享，再到老板畅谈对重庆的认识和想法，太多话题，只恨旅途短暂。</p>
<img src="/2018/06/22/重庆重庆/d3_1.png">
<p>清早离开蒙奇奇，有些不舍，老板在考品酒师，客厅有各种酒，这里也有各式各样的住宿方式，除了双人床上下床这些，还有帐篷、屋内和阳台的沙发，很难估算小小的单元房究竟能住下多少人，收纳多少有趣的灵魂。</p>
<img src="/2018/06/22/重庆重庆/d3_2.png">
<p>网红景点扫荡的差不多，最后一天没有什么目的性，到李子坝站看楼内延伸的轻铁，本来还担心找不到最佳拍摄点，结果出了站一路牌子指引，生怕你找不到，不知说什么好。在重庆永远不怕看不到景点，一堆人加上举起的手机就是最好的导航。在这里拍完查到附近有家不错的鸡店，重庆的鸡有很多种吃法，每一种都很出名，去早了等待时遇到一对北京的新婚夫妇，相互就像遇见了救星，六七斤一只的鸡给了我们拼桌的缘分。饭后消食一行人去了不远的二场，说是不远，也是地图距离，在重庆最不能相信的就是地图。说起来这里很多景区马路边都没有人行通道，渣滓洞是这样，二场更是这样，半路迷路问当地人才发现所谓的人行道就是在居民楼里不断穿越，楼里的楼梯甚至都是一条公用道路，若是一直跟着公路走不知要绕多少冤枉路。</p>
<img src="/2018/06/22/重庆重庆/d3_3.png">
<p>路上碰到当地人说这里是一个北京人建的，瞬间心凉了半截，好像是北京人来验收成果的感觉。果然，七九八的风格，但又有些东施效颦，毕竟七九八真正是在老工业区上改造出来的，这里却是居民楼硬改成旧工业风格，怎么看怎么别扭。</p>
<img src="/2018/06/22/重庆重庆/d3_4.png">
<p>最后特别盘点一下重庆的美食，不愧是火锅底料火了一个城市，只要进了重庆，大街小巷全是火锅底料的味道，然而却不是所有人都有福消受这份美味，大多外地游客挑战微微辣就满脸通红，回去也要难受一夜。还有磁器口的串串、冰粉，解放碑的小龙虾，路边的小面和红油抄手，李子坝的梁山鸡，浓浓的药膳味儿，见一妹子自己吃的风生水起，十分佩服。回来还是带了火锅底料，却没有当地那般辣，大概所有特产都难有原生的滋味。</p>
<img src="/2018/06/22/重庆重庆/d3_5.png">
<p>重庆三天玩遍了地标，不敢说吃遍了美食，却也好好赏了美女，山路及湿润注定了重庆妹子身材纤细、皮肤细腻。但短短三天却来不及感受重庆人真正的样子，这里急求发展，像一座新城，而久居在此的人们是否真的适应重庆的改变，我们只是看了表面，终究无法探得其心。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/22/重庆重庆/" data-id="cjkxqln3o001gnctlct2hbv3b" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/重庆/">重庆</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-VGG-详解" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/22/VGG-详解/" class="article-date">
  <time datetime="2018-06-22T01:01:20.000Z" itemprop="datePublished">2018-06-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学习/">学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/22/VGG-详解/">VGG 详解</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>CONTENT<br><a href="/2018/06/13/CNN基础/" title="CNN-基础">CNN-基础</a><br><a href="/2018/06/12/CNN-入门/" title="CNN入门">CNN入门</a><br><a href="/2018/06/14/AlexNet-详解/" title="AlexNet 详解">AlexNet 详解</a><br><a href="/2018/06/22/VGG-详解/" title="VGG 详解">VGG 详解</a></p>
<hr>
<p>paper <a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">https://arxiv.org/abs/1409.1556</a><br>深度学习网络得益于大规模数据集和计算性能（GPU,分布架构）提升。</p>
<h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><p>主要创新点：深！<br>为了更深，所有层使用更小的滤波器3×3</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><h3 id="滤波层"><a href="#滤波层" class="headerlink" title="滤波层"></a>滤波层</h3><ul>
<li>3×3 filter——保留左右上下中心信息的最小尺寸</li>
<li>1×1 filter——输入的线性变换</li>
<li>stride=1</li>
<li>spatial padding=1 for 3×3 保持分辨率</li>
<li>pooling——5 max-pooling layers(2×2pixel stride 2) 不是每层都有</li>
</ul>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><ul>
<li>前两层各4096channels</li>
<li>最后一层1000soft-max layer</li>
</ul>
<p>所有隐含层使用ReLU,取消了Local Response Normalisation</p>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><img src="/2018/06/22/VGG-详解/1.png">
<p>多个3×3卷积层有与大卷积有同样的receptive field,两个等同5×5，三个等同7×7。优势在于：<br>1、使用3个非线性整流层，使决策函数更具有区别性。<br>2、节省将近一半参数<br>1×1卷积层增加决策函数的非线性，但不影响receptive field。（Network in Network）</p>
<h3 id="框架细节"><a href="#框架细节" class="headerlink" title="框架细节"></a>框架细节</h3><ul>
<li>训练<br>batch size 256<br>momentem 0.9<br>dropout 0.5<br>learning rate 0.01, 10倍下降<br>370K iterations, 74 epochs(多层隐含规范化、小尺寸滤波，某些层预初始)<br>预处理先用浅层网络A做随机初始训练，取前4层和后3层做初始，训练其他网络。<br>训练尺寸采用抖动，图片大小取{256，512}之间，截取224</li>
<li>测试<br>测试尺寸用3个尺寸，截取150测试集取平均。（缩放和随机选取）</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/22/VGG-详解/" data-id="cjkxqln2u0006nctlhat9igj6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-南太行之行" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/21/南太行之行/" class="article-date">
  <time datetime="2018-06-21T01:40:29.000Z" itemprop="datePublished">2018-06-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/游记/">游记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/21/南太行之行/">南太行之行</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>爱一个人或者恨一个人就带他穿越南太行。这里太美，千年地貌、百年绿荫，穿插着山里人的智慧，穿梭在隧道中感受时光的力量。这里太险，徒步走在悬崖边，碎石路，常年落石形成的山道上，坐车行驶在陡峭百转的山路上，与车抢道在忽明忽暗的挂壁公路上。在气势磅礴的自然面前，忘记一切现实的琐碎，眼中只有脚下的路和目之所及的震撼。即便短短两天，却留下深刻美好的记忆，关于风景，关于友情，关于那些有趣美妙的灵魂。</p>
<hr>
<h2 id="第0天"><a href="#第0天" class="headerlink" title="第0天"></a>第0天</h2><p>海淀五路居集合，开往河北新乐市暂停过夜。<br>晚上在附近绕了许久才找到入口，不敢相信住的这么好，置身霍克沃兹一般，很多河北的盆友都不知道石家庄还有这样的酒店。</p>
<img src="/2018/06/21/南太行之行/d0_1.png">
<h2 id="第1天"><a href="#第1天" class="headerlink" title="第1天"></a>第1天</h2><p>继续前往山西晋城，中午出高速后搭当地小面到马武寨，（指路百度百科<a href="https://baike.baidu.com/item/马武寨/10751073" target="_blank" rel="noopener">https://baike.baidu.com/item/马武寨/10751073</a> ），这里开始初揭王莽岭的面纱，绿荫盖赤壁只差无人机。经历过重庆司机上下转弯横冲直撞后，马武寨司机的开车技能更让人惊悚，180度的山道加陡坡，极窄的悬崖路还要不时对向错车，一个颠簸竟看到方向盘的盖子掉下来了！！然而师傅很淡定的装了回去还试了试闷响的喇叭。比起弯道同样夸张的巴拉格宗，这四十分钟车程简直刺激！</p>
<p>下午从马武寨开始穿越，领队大礼包日行10公里，从村里出发经过一片玉米地，不记得什么时候眼前已经是这幅景象。<br><img src="/2018/06/21/南太行之行/d1_1.png"></p>
<p>不是第一次走未开发的山路，在香格里拉千岛山还经历过暴雨，但相比之下，南太行真是太不容易了。各种碎石路、石头路、悬崖边以及没路找路。这次的地陪非常细心，在每个可能出问题的危险地带把控节奏，一遍遍提醒每一脚应该踩在哪，这次能顺利完成有部分靠运气，但大多数还是向导和几位队长的细心。听说别的队有人骨折，山里没有太多劳力，后面受伤的人等了两个小时才等到救援，不禁唏嘘。本人腿不长平衡感还差，很多别人迈腿的地方我只能靠蹦，结果蹦着蹦着右膝就出问题了，第一次感觉到膝盖骨的重要，也无比怀念柏油马路和冰可乐的滋味。</p>
<img src="/2018/06/21/南太行之行/d1_2.png">
<p>深潭这处比较有意思，不是潭水深而是处在极深的石壁间，完全没有像样的路，就靠边上一根横木坐着往下挪，坐着坐着就到底了。北方水少，全靠七八月雨季才能有些积水，看到山间溪流倍加珍惜，未经开发的山间溪水虽然谈不上视觉冲击，但是让人心情舒畅很多，徒步间疲惫、担心都被一路的景色吸收，只想再看一些再记一些。</p>
<img src="/2018/06/21/南太行之行/d1_3.png">
<p>旅伴说这山很有山下问童子的感觉，走过很多城市，很多过度开发的景区，在人群中艰难的留照，倒是忘了旅行本来的感觉。不会有人告诉你在哪里该看什么，全凭自己发现，一抬头便是一线天，一转弯便是高山笼罩，南太行好像很知道人的审美持久性，不断的变换景象，一步一眼，没有功利的前行，愉悦感全在过程中。</p>
<img src="/2018/06/21/南太行之行/d1_4.png">
<p>第一天晚上住在抱犊村（百度百科指路<a href="https://baike.baidu.com/item/抱犊村/8196587?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/抱犊村/8196587?fr=aladdin</a> )，领队说条件不太好，但是却找到了群体感，排队洗澡，好几人一屋上下铺，甚至只能靠木梯子爬上爬下，膝盖有些扭伤，每次下楼都是挑战，有点担心能不能坚持下来第二天的穿越。所有人都很累，但是依然舍不得一夜就这么结束，偶有雨点落下依然凑了两桌狼人杀，激烈的复盘后，寨子终于回归宁静，而我的室友，一位高中语文老师，还在挑灯备课，由衷佩服。</p>
<h2 id="第2天"><a href="#第2天" class="headerlink" title="第2天"></a>第2天</h2><p>五点半被隔壁夕阳红团吵醒，暗叹一句我还年轻，膝盖好一些了小惊喜。第二天开始向锡崖沟穿越（百度百科指路<a href="https://baike.baidu.com/item/锡崖沟景区/1156617?fromtitle=%E9%94%A1%E5%B4%96%E6%B2%9F&amp;fromid=1842763&amp;fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/锡崖沟景区/1156617?fromtitle=%E9%94%A1%E5%B4%96%E6%B2%9F&amp;fromid=1842763&amp;fr=aladdin</a> ），领队说今天路平坦很多，大舒一口气，后来再看还真是太年轻了啊！刚出发不久就听到咩咩的叫声，碰到了一群带角的小白小黑，从队友的作品来看，泥萌也太会找镜头了吧，社会社会！<br><img src="/2018/06/21/南太行之行/d2_1.png"></p>
<p>今天的画风又是完全不同，一路穿行在悬崖边却只在感叹大自然的鬼斧天工，坐在悬崖边拍照的小伙伴都害怕，往脚下看，娘呦我都干了啥，怂怂的赶紧又缩回去了。途中居然有块指向标，待走到景区就餐看到观光梯后发现野生路线实在是太爽了，商业难窥它十分之一的美。</p>
<img src="/2018/06/21/南太行之行/d2_2.png">
<p>不紧不慢走在山间，行在两省边界处，一会儿进入山西一会儿又跨到河南，感觉很奇妙，太行山不仅展示了它的磅礴也秀出它秀气的一面，有队友还拍到了猴子，这两天攀爬跳跃强烈意识到果然是由灵长类演化而来，看起来更亲切了一些。</p>
<img src="/2018/06/21/南太行之行/d2_3.png">
<p>徒步不总是愉悦的，有时会有些无聊，有时会陷入迷茫，但你总需要走一遍才能真正知道收获了什么。</p>
<img src="/2018/06/21/南太行之行/d2_4.png">
<p>至此还是比较顺畅的，膝盖不能打弯的形况下平路是多么友好，直到。。。看到。。。下面这块牌子。。。<br>画！风！崩！了！</p>
<img src="/2018/06/21/南太行之行/d2_5.png">
<p>没路啊盆友们！坡度在60以上，全靠踩下去的一些浅坑和落石组合，连扑带爬还要时不时停在路中等前面人先动，前后40多分钟，所有人都沉默了，只有一件事，爬！地陪还是很给力的，爬了一半多的时候就提醒我们整休，担心平常不运动会中暑，事实证明，我们还是可以的。现在想想这么多人安全回来多么不可思议！</p>
<img src="/2018/06/21/南太行之行/d2_6.png">
<p>很多时候成就感来源于超越自己，同样的风景在艰难的跋涉后再看又是另一番滋味，登顶后只想大喊一声“还！有！谁！”。身后是一条很长的隧道，凉风袭来赶走了疲惫和汗水，最重要的是以为有车来接我们下山了，然而，，，呵呵。</p>
<img src="/2018/06/21/南太行之行/d2_7.png">
<p>这是一条将近1公里的隧道，虽然也有车上来接游客，但不同于一般的穿山隧道，它没有灯。走在黑漆漆的洞中，想起知乎里面关于探洞的讨论身有感触。当某个感官下降时，其他会变得更灵敏，身处黑暗，皮肤更懂得清凉，慢慢寻得出口，真正有了一翻穿越实感，回首望去，与自然相比我们无限渺小。</p>
<img src="/2018/06/21/南太行之行/d2_8.png">
<p>午后南太行大概觉得自己还不够美，起了一波太阳，晴看水雾看山，山水总有它不经意的美，但有了一些阳光竟又被惊艳到，山谷的魔力让人永远看不够。</p>
<img src="/2018/06/21/南太行之行/d2_9.png">
<p>到锡崖沟休整了一会儿，晚饭前攀爬挂壁公路，回程时有队友介绍了这段公路背后的故事，非常震撼。比起愚公移山，亲眼见到并走过锡崖沟人在悬崖峭壁上开凿的这条公路，才感到人的精神力量竟可与自然抗衡。</p>
<img src="/2018/06/21/南太行之行/d2_10.png">
<p>走在挂壁公路上忽明忽暗，路上只能看到崖壁的纹理，等上到观景台时才能看到整段公路的美，尤其是赶上车队在山中穿行，到了观景台已经没了拍照兴致，静静坐在台子上发呆，看山看车看村落，思想飘一会儿。<br>这里发生了一段小插曲，想起来十分后怕，上面一个观景台掉下一个玻璃杯，磕在石壁上碎开落下，下面小伙伴在玻璃渣中狂窜，触目惊心。</p>
<img src="/2018/06/21/南太行之行/d2_11.png">
<p>回去路上有个小伙伴向一辆公交车敬了个礼，然后司机竟然懂了！锡崖沟人民真是力量与智慧并存啊，还有这位小伙伴你怎么能这么优秀！于是坐着车唱着歌开心回到村子里。这里的村落可谓依山傍水，遇到一个钓鱼老翁，看上去都有了诗意。</p>
<img src="/2018/06/21/南太行之行/d2_12.png">
<p>在此要夸一下领队，虽然出门在外跋山涉水，却一点没落下节日的气息，一群不相识却有着共同爱好的人在端午的日子聚到一起，天亮游山天黑杀人，最妙的是还能放烟花包粽子，从来没想过端午可以过得这么充实难忘。</p>
<img src="/2018/06/21/南太行之行/d2_13.png">
<p>最后一天返京，虽然是将近10个小时的车程，却一点不觉得长，听着才华横溢的伙伴们自我介绍，羞涩的歌声和互相挑逗，这就是所谓有趣的灵魂终将相遇，一起开拓前方未知的道路。感谢领队、感谢小伙伴们、感谢为了我们辛苦工作的服务人员，全部收进回忆里珍藏。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/21/南太行之行/" data-id="cjkxqln3e000wnctlpfmhynn5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/南太行/">南太行</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-新疆攻略" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/20/新疆攻略/" class="article-date">
  <time datetime="2018-06-20T01:17:44.000Z" itemprop="datePublished">2018-06-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/游记/">游记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/20/新疆攻略/">新疆攻略</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://www.mafengwo.cn/gonglve/ziyouxing/5773.html" target="_blank" rel="noopener">http://www.mafengwo.cn/gonglve/ziyouxing/5773.html</a><br><a href="http://www.mafengwo.cn/gonglve/ziyouxing/1661.html" target="_blank" rel="noopener">http://www.mafengwo.cn/gonglve/ziyouxing/1661.html</a> </p>
<h2 id="地图篇"><a href="#地图篇" class="headerlink" title="地图篇"></a>地图篇</h2><img src="/2018/06/20/新疆攻略/map1.png">
<p>天山以北为北疆，天山以南为南疆。<br><img src="/2018/06/20/新疆攻略/map2.png"></p>
<ul>
<li>乌鲁木齐周边：红山公园，水磨访，二道桥国际大巴扎、天山天池、吐鲁番、南山牧场；建议游玩2-3天。</li>
<li>南线：库尔勒、轮台、库车、喀什、香妃墓、达瓦昆沙漠、帕米尔高原、艾提尕尔清真寺；建议游玩7-8天。</li>
<li>北线：可可托海、五彩滩、五彩城、白哈巴、禾木、喀纳斯、乌禾魔鬼城；建议游玩5-6天。</li>
<li>西线：塞里木湖、霍尔果斯口岸、伊宁、那拉提、巴音布鲁克；建议游玩：4-5天。</li>
<li>东线：鄯善、库木塔格沙漠；建议游玩1-2天。<br>新疆总的分为南疆和北疆，上面所述的北线、西线、东线统称为北疆，主要的景点都集中在北疆，这也是游客选择最多的线路。</li>
</ul>
<h2 id="北疆"><a href="#北疆" class="headerlink" title="北疆"></a>北疆</h2><p>北疆以大自然风光为主，比较著名的城市和地区有：乌鲁木齐、吐鲁番、伊犁、阿勒泰、塔城、博尔塔拉等都是属于北疆。整个北疆拥有悠长的牧民文化、高山草原，还有奔驰的骏马与专属新疆特有的歌声。<br>市内一天，天山天池一天，吐鲁番一天，北疆四天，西疆四天</p>
<h3 id="乌鲁木齐"><a href="#乌鲁木齐" class="headerlink" title="乌鲁木齐"></a>乌鲁木齐</h3><p>天山天池：<br>天山明珠，整个湖水晶莹剔透，四周环绕着群山，湖面倒影远处的博格达峰，岸边还有提拔苍翠的云杉，风景宛如仙境，宛如众山环绕下的蓝宝石。<br>游玩路线一般由两个部分，一是沿着天山天池湖边的栈道一直缓慢步行前进，一路拾阶而上，便可看到整个天池主湖的风光了！二是沿着瀑布向下环游，你也可以坐上一艘小船进入天池中体验看风景的惬意。<br>全程游览时间约4-5小时。 整体车程约3个半小时。 客运站班车。</p>
<p>乌鲁木齐老城——新疆国际大巴扎</p>
<h3 id="吐鲁番"><a href="#吐鲁番" class="headerlink" title="吐鲁番"></a>吐鲁番</h3><p>据乌鲁木齐190公里，古丝绸之路重要位置。<br>西天取经唐僧师徒走过的火焰山、苏公塔葡萄沟、坎儿井、高昌古城等特有的别致景点。 建议包车往返。<br>火车1小时：<br>D8806：16:20-17:20（头一天晚上抵达）<br>D8808：20:17-21:17（头一天晚上抵达）<br>D8802：09:06-10:04（当天抵达）（推荐）<br>D2704：09:32-10:30（当天抵达） </p>
<p>最低盆地，艾丁湖、葡萄沟、火焰山、坎儿井、库木塔格沙漠</p>
<h3 id="喀纳斯"><a href="#喀纳斯" class="headerlink" title="喀纳斯"></a>喀纳斯</h3><p>人间仙境，是中国最美湖泊的地方，湖水清澈见底，蜿蜒穿过原始森林，焕幻的光影交织在岸边金黄墨绿的树木，仿佛置身梦境中的瑞士风光中。 集中了新疆的冰川、森林、牧场、草原、湖泊河流、民族风情等特色为一体。<br>第1天：北屯一布尔津一五彩滩一白哈巴，宿白哈巴村<br>第2天：白哈巴一喀纳斯（景区内穿越），宿喀纳斯景区内小木屋<br>第3天：喀纳斯一贾登峪一禾木，宿禾木村<br>第4天：禾木一北屯 </p>
<p>第1天：乌鲁木齐—五彩滩—布尔津（单程约750公里，行车约10小时）<br>第2天：布尔津—喀纳斯湖—贾登峪（单程180公里，行车约4小时）<br>第3天：贾登峪—禾木—布尔津（单程180公里山路，行车约4小时）<br>第4天：布尔津—乌尔禾魔鬼城—乌鲁木齐（单程约750公里，行车约10小时）<br><img src="/2018/06/20/新疆攻略/喀纳斯1.png"><br>推荐的观赏景点有：喀纳斯湖、神仙湾、月亮湾、卧龙湾等。<br>乌鲁木齐到北屯市，可以乘坐飞机前往，或者可以选择火车到北屯。  </p>
<h3 id="伊犁"><a href="#伊犁" class="headerlink" title="伊犁"></a>伊犁</h3><p>以大为美，现如今流传着一句话：不到新疆，不知中国之大；不到伊犁，不知新疆之美。这个伊犁的景色四季分明，定会让你心驰神往的！ 大片的草原和森林是伊犁最大的特色。<br>第一天：伊宁→那拉提（全程约280公里，乘车用时约4小时）<br>第二天：伊宁市→喀拉峻→昭苏（全程约280公里，乘车用时约4小时，全程一级路面）<br>第三天：昭苏→伊宁市（全程约200公里，乘车用时约3小时，全程一级路面）<br>第四天：伊宁市→霍尔果斯口岸→赛里木湖→伊宁市（全程约300公里，乘车用时约5小时，全程一级路面） </p>
<p>第一天：乌市—塞里木湖—清水河 （约640公里约9小时）<br>第二天：清水河—那拉提 （约320公里约6小时）<br>第三天：那拉提—伊宁（约260公里约4小时）<br>第四天：伊宁—乌市 （约720公里约9小时）<br><img src="/2018/06/20/新疆攻略/伊宁1.png"><br>推荐景点：巩乃斯、那拉提、唐布拉、巴音布鲁克、昭苏、喀拉峻等</p>
<h3 id="长线游"><a href="#长线游" class="headerlink" title="长线游"></a>长线游</h3><p>1、10日游<br>第一天：乌鲁木齐-布尔津（单程约680公里，行车约9小时）<br>第二天：布尔津-贾登峪-喀纳斯 （约180公里，行车约3小时）<br>第三天：贾登峪-禾木 (约90公里 行车约2小时)<br>第四天：禾木—布尔津 （约200公里 行车约4小时）<br>第五天：布尔津—博乐/精河 （590公里，行车约10小时）<br>第六天：博乐—那拉提（ 465公里，行车约7小时）<br>第七天：那拉提镇-那拉提空中草原-巴音布鲁克（86公里，行车约2小时）<br>第八天：巴音布鲁克-吐鲁番（505公里，行车约9小时）<br>第九天：吐鲁番—鄯善沙漠—乌鲁木齐（280公里，行车约4小时）<br>第十天：乌鲁木齐-天山天池（120公里，行车约1.5小时） </p>
<h3 id="自驾"><a href="#自驾" class="headerlink" title="自驾"></a>自驾</h3><p><a href="http://you.ctrip.com/travels/urumqi117/3174883.html" target="_blank" rel="noopener">http://you.ctrip.com/travels/urumqi117/3174883.html</a> 2016年9月北疆10日自驾游（3600公里，独自驾驶）<br><a href="http://www.zyfzyx.com/xjkx/index.html" target="_blank" rel="noopener">http://www.zyfzyx.com/xjkx/index.html</a> 自由行顾问推荐</p>
<h2 id="南疆"><a href="#南疆" class="headerlink" title="南疆"></a>南疆</h2><p>南疆以历史文化为主，涵盖了喀什、和田、阿克苏、巴音郭楞、克孜勒苏柯尔克孜等地，其中还包含了吐鲁番和塔里木盆地、以及昆仑山脉新疆部分。来到南疆，你会看到沙漠与戈壁的完美结合，还有多元淳朴的农业文明，感受木卡姆与舞蹈的律动。 5月份，南疆地区，比如喀什、库尔勒、库车、和田等地区，早已经是春暖花开，是去南疆的好时候。喀什丰富多彩的民族风情、帕米尔高原的辽阔壮美；和田昆仑山下的巍峨秀丽、和田民风的淳朴自然；库车、库尔勒的沙漠风光、千年胡杨和神秘大峡谷，等等，非常的迷人。</p>
<h2 id="时间线"><a href="#时间线" class="headerlink" title="时间线"></a>时间线</h2><ul>
<li>第1天 中午到新疆</li>
<li></li>
</ul>
<p>天津-&gt;乌鲁木齐 飞机<br>乌鲁木齐-&gt;阿勒泰 火车 21-23 7-8 1天<br>阿勒泰3天<br>阿勒泰-&gt;？-&gt;伊宁 1天<br>伊宁3天<br>伊宁-&gt;乌鲁木齐 21-23 7<br>天山大峡谷/天池<br>乌鲁木齐-&gt;北京 买买买 飞机 </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/20/新疆攻略/" data-id="cjkxqln3g0010nctl6ns7gfmv" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/新疆/">新疆</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-AlexNet-详解" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/14/AlexNet-详解/" class="article-date">
  <time datetime="2018-06-14T09:14:02.000Z" itemprop="datePublished">2018-06-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学习/">学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/14/AlexNet-详解/">AlexNet 详解</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>CONTENT<br><a href="/2018/06/13/CNN基础/" title="CNN-基础">CNN-基础</a><br><a href="/2018/06/12/CNN-入门/" title="CNN入门">CNN入门</a><br><a href="/2018/06/14/AlexNet-详解/" title="AlexNet 详解">AlexNet 详解</a><br><a href="/2018/06/22/VGG-详解/" title="VGG 详解">VGG 详解</a></p>
<hr>
<p>paper <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Imagenet大数据集，120万图片，1000类。6千万参数，65万神经元。</p>
<hr>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><img src="/2018/06/14/AlexNet-详解/1.png">
<p>5个卷积层，3个全连接层，最后使用1000-way softmax.</p>
<ul>
<li>第一层 96个11×11×3 normalization+maxpooling</li>
<li>第二层 256个5×5×48 normalization+maxpooling 交换传输至第三层</li>
<li>第三层 384个3×3×256 </li>
<li>第四层 384个3×3×192</li>
<li>第五层 256个3×3×192 maxpooling 交换传输到第六层</li>
<li>第六层 4096个neurons dropout</li>
<li>第七层 4096个newrons dropout</li>
<li>第八层 1000way softmax</li>
</ul>
<hr>
<h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><h3 id="ReLU激活函数"><a href="#ReLU激活函数" class="headerlink" title="ReLU激活函数"></a>ReLU激活函数</h3><p>特点：1、计算快，不需要除法 2、非饱和函数迭代次数少<br><img src="/2018/06/14/AlexNet-详解/2.png"><br>明显加快训练速度。</p>
<h3 id="多GPU训练"><a href="#多GPU训练" class="headerlink" title="多GPU训练"></a>多GPU训练</h3><p>单个GTX580只有3GB存储，限制了网络参数数量，使用两个GPU训练，将一半核部署在单个GPU上，前后连接有单连接和交叉连接，通过交叉验证方式确定不同层之间的连接方式。</p>
<h3 id="Local-Response-Normalization"><a href="#Local-Response-Normalization" class="headerlink" title="Local Response Normalization"></a>Local Response Normalization</h3><p>ReLU不需要输入规范化，这里规范化是为了模拟人脑神经单元增加横向抑制。<br><img src="/2018/06/14/AlexNet-详解/3.png"><br>附近的几个核做归一化。</p>
<h3 id="Overlapping-Pooling"><a href="#Overlapping-Pooling" class="headerlink" title="Overlapping Pooling"></a>Overlapping Pooling</h3><p>移动间距小于核的尺寸，防止过拟合</p>
<hr>
<h2 id="减少过拟合的方法"><a href="#减少过拟合的方法" class="headerlink" title="减少过拟合的方法"></a>减少过拟合的方法</h2><h3 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h3><p>1、Label-perserving transformations<br>原图像256×256<br>训练集随机抽取其中224×224，再进行垂直翻转，扩大2048倍<br>测试机选取四角及中心小图像，再进行垂直翻转，扩大10倍<br>2、调整亮度<br>训练集数据则更加主成分<br><img src="/2018/06/14/AlexNet-详解/4.png"><br>p和λ是RGB元素协方差矩阵的特征值和特征向量，α是随机数，每个元素加上该向量，保留内部特征，以便训练得到无关光照强度和颜色的本质特征。</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>类似于随机森林，通过一系列若分类器叠加得到一强分类器。<br>由于深度神经网络很庞大，采取折中的形式，在每次迭代中，每个隐藏元有0.5的概率输出为0，不参与前向和反向传播。<br>这样减少了神经元之间复杂的协同适应性，神经元不能依赖于某些特定的神经元的表现。<br>代价是迭代次数double.<br>在测试节点，输出需要×0.5</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/14/AlexNet-详解/" data-id="cjkxqln2n0004nctlzpj1ribu" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CNN基础" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/13/CNN基础/" class="article-date">
  <time datetime="2018-06-13T08:11:03.000Z" itemprop="datePublished">2018-06-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学习/">学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/13/CNN基础/">CNN-基础</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>记录CNN中用到的基本概念、数学模型等</p>
<hr>
<h2 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h2><p>解决的是多分类问题，逻辑回归是二分类的softmax回归<br>参考：<a href="https://www.cnblogs.com/bzjia-blog/p/3366780.html" target="_blank" rel="noopener">https://www.cnblogs.com/bzjia-blog/p/3366780.html</a></p>
<img src="/2018/06/13/CNN基础/softmax1.png">
<p>θ用一个k×(n+1)的矩阵<br>代价函数如下：<br><img src="/2018/06/13/CNN基础/softmax2.png"><br>最后加一个权重衰减</p>
<hr>
<h2 id="Receptive-Field"><a href="#Receptive-Field" class="headerlink" title="Receptive Field"></a>Receptive Field</h2><p>在CNN中，第n层特征图中一个像素，对应第1层（输入图像）的像素数，即为该层的Receptive Field，简称RF。<br>参考：<a href="https://blog.csdn.net/shenxiaolu1984/article/details/78815922" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/78815922</a><br>    <a href="https://blog.csdn.net/baidu_32173921/article/details/70049186" target="_blank" rel="noopener">https://blog.csdn.net/baidu_32173921/article/details/70049186</a><br><img src="/2018/06/13/CNN基础/RF1.png"></p>
<ul>
<li>一层卷积层的输出特征图像素的感受野的大小等于滤波器的大小</li>
<li>深层卷积层的感受野大小和它之前所有层的滤波器大小和步长有关系</li>
<li>计算感受野大小时，忽略了图像边缘的影响，即不考虑padding的大小</li>
</ul>
<h2 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h2><p>填充是指在图像之间添加额外的零层，以使输出图像的大小与输入相同。这被称为相同的填充。<br><img src="/2018/06/13/CNN基础/padding1.png"><br><a href="https://blog.csdn.net/pangjiuzala/article/details/72630166" target="_blank" rel="noopener">https://blog.csdn.net/pangjiuzala/article/details/72630166</a><br>在应用滤波器之后，在相同填充的情况下，卷积层具有等于实际图像的大小。<br>有效填充是指将图像保持为具有实际或“有效”的图像的所有像素。在这种情况下，在应用滤波器之后，输出的长度和宽度的大小在每个卷积层处不断减小。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>参考：<a href="https://zhuanlan.zhihu.com/p/24018768" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24018768</a><br><img src="/2018/06/13/CNN基础/LSTM1.jpg"><br>A 代表神经网络主体, xt 是网络输入，ht是网络输出，循环结构允许信息从当前输出传递到下一次的网络输入。 </p>
<p>RNN的问题：人们希望RNNs能够连接之前的信息到当前的任务中，例如，使用之前的图像帧信息去辅助理解当前的帧。不幸的是，随着间隔的增大，RNNs连接上下文信息开始力不从心了。理论上RNNs完全有能力处理这种“长期依赖（Long-term dependencies）”问题。人们可以精心的选择参数去接着这类问题。令人沮丧的是，实践表明RNNs不能完美的学习“长期依赖（Long-term dependencies）”。Hochreiter和Bengio发现了一些为什么RNNs在这些问题上学习相当困难的根本原因。<br>谢天谢地，LSTMs没有这些问题。</p>
<p>LSTM长短期记忆网络，记住长时间段的信息是他们的必备技能。所有的递归神经网络都有重复神经网络本身模型的链式形式。在标准的RNNs， 这个复制模块只有一个非常简单的结构，例如一个双极性（tanh）层。<br><img src="/2018/06/13/CNN基础/LSTM2.jpg"></p>
<p>LSTMs 也有这种链式结构，但是这个重复模块与上面提到的RNNs结构不同：LSTMs并不是只增加一个简单的神经网络层，而是四个，它们以一种特殊的形式交互。<br><img src="/2018/06/13/CNN基础/LSTM3.jpg"><br>神经元状态Ct就像是一个传送带。它的线性作用很小，贯穿整个链式结构。信息很容易在传送带上传播，状态却并不会改变。<br><img src="/2018/06/13/CNN基础/LSTM4.jpg"><br>LSTM有能力删除或者增加神经元状态中的信息，这一机制是由被称为门限的结构精心管理的。门限是一种让信息选择性通过的方式，它们是由Sigmoid神经网络层和逐点相乘器做成的。 Sigmod层输出0~1之间的数字，描述了一个神经元有多少信息应该被通过。输出“0”意味着“全都不能通过”，输出“1”意味着“让所有都通过”。一个LSTM有三个这样的门限，去保护和控制神经元状态。</p>
<ul>
<li><p>遗忘门层<br>LSTM的第一步就是决定什么信息应该被神经元遗忘。<br>它输入ht−1和xt,然后在Ct−1的每个神经元状态输出0~1之间的数字。“1”表示“完全保留这个”，“0”表示“完全遗忘这个”。 回到那个尝试去根据之前的词语去预测下一个单词的语言模型。在这个问题中，神经元状态或许包括当前主语中的性别信息，所以可以使用正确的代词。当我们看到一个新的主语，我们会去遗忘之前的性别信息。 </p>
<img src="/2018/06/13/CNN基础/LSTM5.jpg">
</li>
<li><p>输入门层<br>决定我们要在神经元细胞中保存什么信息。<br>这包括两个部分。首先，一个被称为“输入门层”的Sigmod层决定我们要更新的数值。然后，一个tanh层生成一个新的候选数值，Ct˜,它会被增加到神经元状态中。在那个语言模型例子中，我们想给神经元状态增加新的主语的性别，替换我们将要遗忘的旧的主语。 </p>
<img src="/2018/06/13/CNN基础/LSTM6.jpg">
</li>
</ul>
<p>组合这两步去生成一个更新状态值。<br>更新旧的神经元状态Ct−1到新的神经元状态Ct了。给旧的状态乘以一个ft,遗忘掉我们之前决定要遗忘的信息，然后我们增加it∗Ct˜。这是新的候选值，是由我们想多大程度上更新每个状态的值来度量的。在语言模型中，就像上面描述的，这是我们实际上要丢弃之前主语的性别信息，增加新的主语的性别信息的地方。<br><img src="/2018/06/13/CNN基础/LSTM7.jpg"></p>
<ul>
<li>输出门层<br>决定要输出什么。<br>这个输出是建立在我们的神经元状态的基础上的，但是有一个滤波器。首先，我们使用Sigmod层决定哪一部分的神经元状态需要被输出；然后我们让神经元状态经过tanh（让输出值变为-1~1之间）层并且乘上Sigmod门限的输出，我们只输出我们想要输出的。 对于那个语言模型的例子，当我们看到一个主语的时候，或许我们想输出相关动词的信息，因为动词是紧跟在主语之后的。<img src="/2018/06/13/CNN基础/LSTM8.jpg">
</li>
</ul>
<p>未来趋势<br>是否有比LSTMs更好的模型？学者一致认为：“有的！这里有下一步，它就是“注意力”！”（Yes! There is a next step and it’s attention!，这里的”attention”翻译成“注意力”不知道是否合适”?）一个观点是让RNN的每一步都监视一个更大的信息集合，并从中挑选信息。例如：如果你使用 RNN去为一幅图像生成注释，它会从图像中挑选中挑选一部分去预测输出单词。实际上，Xu, et al. (2015) 确实是这样做的–如果你想去探索“注意力”，这或许是一个有趣的起点！这里还有一些使用“注意力”得到的有趣的结果,并且还有更多人在使用这个。<br>“注意力”并不是唯一的RNN研究热点。格点LSTMs也看起来很有趣。最近几年递归神经网络很流行，从趋势来看，未来还会更流行。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/13/CNN基础/" data-id="cjkxqln2q0005nctlg0yy2ltd" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CNN-入门" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/12/CNN-入门/" class="article-date">
  <time datetime="2018-06-12T02:13:17.000Z" itemprop="datePublished">2018-06-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学习/">学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/12/CNN-入门/">CNN入门</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>从走马观花到玩家。</p>
<hr>
<h2 id="CNN经典模型"><a href="#CNN经典模型" class="headerlink" title="CNN经典模型"></a>CNN经典模型</h2><p>参考：</p>
<ul>
<li>1、 <a href="https://yq.aliyun.com/articles/176612" target="_blank" rel="noopener">https://yq.aliyun.com/articles/176612</a> CNN发展历程</li>
</ul>
<h3 id="模型对比"><a href="#模型对比" class="headerlink" title="模型对比"></a>模型对比</h3><p><a href="https://arxiv.org/pdf/1605.07678.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1605.07678.pdf</a> AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS</p>
<img src="/2018/06/12/CNN-入门/compare.png">
<p><a href="https://blog.csdn.net/u010402786/article/details/52433324" target="_blank" rel="noopener">https://blog.csdn.net/u010402786/article/details/52433324</a> Inception解析</p>
<img src="/2018/06/12/CNN-入门/演化图.png">
<h3 id="LeNet5-by-LeCun-1998-5层"><a href="#LeNet5-by-LeCun-1998-5层" class="headerlink" title="LeNet5 by LeCun 1998 5层"></a>LeNet5 by LeCun 1998 5层</h3><p>用到了CNN三个特性：1. 局部感知、2. 下采样、3. 权值共享。</p>
<img src="/2018/06/12/CNN-入门/lenet.png" title="lenet">
<p>创新：</p>
<ul>
<li>卷积神经网络使用 3 层架构：卷积、下采样、非线性激活函数</li>
<li>使用卷积提取图像空间特征</li>
<li>下采样使用了图像平均稀疏性</li>
<li>激活函数采用了 tanh 或者 sigmoid 函数</li>
<li>多层神经网络（MLP）作为最后的分类器</li>
<li>层之间使用稀疏连接矩阵，以避免大的计算成本</li>
</ul>
<p>1998-2010有一段GAP， 随着CPU和GPU速度不断提升，在性能和存储上的飞跃，让深度神经网络更加可算，开启新大门</p>
<h3 id="AlexNet-by-Alex-2012-8层"><a href="#AlexNet-by-Alex-2012-8层" class="headerlink" title="AlexNet by Alex 2012 8层"></a>AlexNet by Alex 2012 8层</h3><p>思想：大的卷积核能捕获图像当中相似的特征<br>特点：可以学习更复杂的图像高维特征</p>
<img src="/2018/06/12/CNN-入门/alex.png">
<p>创新：</p>
<ul>
<li>使用 ReLU 函数作为激活函数，降低了 Sigmoid 类函数的计算量</li>
<li>利用 dropout 技术在训练期间选择性地剪掉某些神经元，避免模型过度拟合</li>
<li>引入 max-pooling 技术</li>
<li>利用双 GPU NVIDIA GTX 580 显著减少训练时间</li>
</ul>
<h3 id="VGG-by-牛津大学-2014-11-19层"><a href="#VGG-by-牛津大学-2014-11-19层" class="headerlink" title="VGG by 牛津大学 2014 11~19层"></a>VGG by 牛津大学 2014 11~19层</h3><p>在每个卷积层使用更小的3*3卷积核，连续使用小的卷积核对图像进行多次卷积。</p>
<img src="/2018/06/12/CNN-入门/vgg.png" title="vgg">
<p>创新：</p>
<ul>
<li>使用多个小卷积核串联模仿大卷积核对图像进行局部感知。</li>
<li>训练时间变长，但是预测时间及参数都减少了。</li>
</ul>
<h3 id="Inception系列-by-google"><a href="#Inception系列-by-google" class="headerlink" title="Inception系列 by google"></a>Inception系列 by google</h3><p>GoogLeNet之前，主流网络追求更多更宽，增大网络有如下缺点：</p>
<ul>
<li>参数太多，容易过拟合，若训练数据集有限；</li>
<li>网络越大计算复杂度越大，难以应用；</li>
<li>网络越深，梯度越往后穿越容易消失（梯度弥散），难以优化模型</li>
</ul>
<h4 id="GoogLeNet（Inception-V1）2014"><a href="#GoogLeNet（Inception-V1）2014" class="headerlink" title="GoogLeNet（Inception V1）2014"></a>GoogLeNet（Inception V1）2014</h4><p>Inception v1的网络，将1x1，3x3，5x5的conv和3x3的pooling，堆叠在一起，一方面增加了网络的width，另一方面增加了网络对尺度的适应性.<br>5×5的卷积核所需的计算量就太大了，造成了特征图厚度很大。在3x3前，5x5前，max pooling后分别加上了1x1的卷积核起到了降低特征图厚度的作用，也就是Inception v1的网络结构。 </p>
<img src="/2018/06/12/CNN-入门/googlenet1.png">
<img src="/2018/06/12/CNN-入门/googlenet.png">
<p>使用1*1卷积核（NiN）减少后续并行操作的特征数量，称为”bottleneck layer”，操作数减少10倍。<br>成功原因是输入特征相关，通过1*1的卷积组合去冗余。<br>能够有效地减少计算资源，在相同的计算成本下，有更好的性能提升框架。</p>
<h4 id="Inception-V2-2015"><a href="#Inception-V2-2015" class="headerlink" title="Inception V2 2015"></a>Inception V2 2015</h4><p>在 googleNet 中加入一个Batch-normalized层<br>另外一方面学习VGG用2个3x3的conv替代inception模块中的5x5，既降低了参数数量，也加速计算； </p>
<img src="/2018/06/12/CNN-入门/inception3.png">
<p>思想：</p>
<ul>
<li>通过构建平衡深度和宽度的网络，最大化网络的信息流。在进入 pooling 层之前增加 feature maps</li>
<li>当网络层数深度增加时，特征的数量或层的宽度也相对应地增加</li>
<li>在每一层使用宽度增加以增加下一层之前的特征的组合</li>
<li>只使用 3x3 卷积</li>
</ul>
<h3 id="ResNet-2015"><a href="#ResNet-2015" class="headerlink" title="ResNet 2015"></a>ResNet 2015</h3><p>输出的是两个连续的卷积层，并且输入时绕到下一层去。</p>
<img src="/2018/06/12/CNN-入门/resnet.png">
<p>绕过 2 层可以看做是在网络中的一个小分类器, 通过这种架构最后实现了神经网络超过1000层。<br>该层首先使用 1x1 卷积然后输出原来特征数的 1/4，然后使用 3×3 的卷积核，然后再次使用 1x1 的卷积核,输出的特征数为原来输入的大小。与BottleneckLayer类似大量地减少了计算量，但是却保留了丰富的高维特征信息。</p>
<ul>
<li>ResNet 可以被看作并行和串行多个模块的结合</li>
<li>ResNet 上部分的输入和输出一样，所以看上去有点像 RNN，因此可以看做是一个更好的生物神经网络的模型</li>
</ul>
<h3 id="SqueezeNet-2016"><a href="#SqueezeNet-2016" class="headerlink" title="SqueezeNet 2016"></a>SqueezeNet 2016</h3><p>有着跟 AlexNet 一样的精度，参数却比 Alex 少了接近 50 倍并且参数只需要占用很小的内存空间。</p>
<img src="/2018/06/12/CNN-入门/squeezenet.png">
<h3 id="Xception-36层"><a href="#Xception-36层" class="headerlink" title="Xception 36层"></a>Xception 36层</h3><p>与 ResNet 和 Inception V4 一样简单且优雅的架构，并且改进了 Inception 模型。<br>与 ResNet-34 非常相似。但是模型的代码与 ResNet 一样简单，并且比 Inception V4 更容易理解。</p>
<img src="/2018/06/12/CNN-入门/xception.png">

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/12/CNN-入门/" data-id="cjkxqln2i0001nctlzvjxmzef" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hexo入门" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/11/hexo入门/" class="article-date">
  <time datetime="2018-06-11T07:17:16.000Z" itemprop="datePublished">2018-06-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/学习/">学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/11/hexo入门/">hexo入门</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>即日起养成随时记录经常总结的好习惯，开始！</p>
<h2 id="hexo-安装"><a href="#hexo-安装" class="headerlink" title="hexo 安装"></a>hexo 安装</h2><p><a href="https://www.cnblogs.com/visugar/p/6821777.html" target="_blank" rel="noopener">https://www.cnblogs.com/visugar/p/6821777.html</a> hexo从零开始到搭建完整</p>
<h2 id="hexo-使用"><a href="#hexo-使用" class="headerlink" title="hexo 使用"></a>hexo 使用</h2><h3 id="设置分类和标签"><a href="#设置分类和标签" class="headerlink" title="设置分类和标签"></a>设置分类和标签</h3><p><a href="http://cache.baiducontent.com/c?m=9f65cb4a8c8507ed4fece763104d8f3d4a09d0347cc0d0622a8fd512d4344c41303dbea627201302cec7767a03a85406bdba6b6f6d546aea8cc8fe48dba6866d72c8713b2a4bcc1c05d368fe980072947dd011&amp;p=8b2a9715d9c846b943b3d6601b49&amp;newp=8a769a4786cc4aa84ba2c10c7f4992695d0fc20e3cd0c44324b9d71fd325001c1b69e7bc23231b04d5c17a6003a84a56e1f33278341766dada9fca458ae7c4&amp;user=baidu&amp;fm=sc&amp;query=hexo+category&amp;qid=cf44458300053267&amp;p1=4" target="_blank" rel="noopener">http://cache.baiducontent.com/c?m=9f65cb4a8c8507ed4fece763104d8f3d4a09d0347cc0d0622a8fd512d4344c41303dbea627201302cec7767a03a85406bdba6b6f6d546aea8cc8fe48dba6866d72c8713b2a4bcc1c05d368fe980072947dd011&amp;p=8b2a9715d9c846b943b3d6601b49&amp;newp=8a769a4786cc4aa84ba2c10c7f4992695d0fc20e3cd0c44324b9d71fd325001c1b69e7bc23231b04d5c17a6003a84a56e1f33278341766dada9fca458ae7c4&amp;user=baidu&amp;fm=sc&amp;query=hexo+category&amp;qid=cf44458300053267&amp;p1=4</a> </p>
<h3 id="同步到github"><a href="#同步到github" class="headerlink" title="同步到github"></a>同步到github</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo g</span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure>
<h3 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h3><p>将图片放到同名文件夹中，输入如下</p>
<img src="/2018/06/11/hexo入门/插入图片.png" title="插入图片">
<h2 id="markdown-语法"><a href="#markdown-语法" class="headerlink" title="markdown 语法"></a>markdown 语法</h2><p><a href="https://www.cnblogs.com/liugang-vip/p/6337580.html" target="_blank" rel="noopener">https://www.cnblogs.com/liugang-vip/p/6337580.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/11/hexo入门/" data-id="cjkxqln2d0000nctlywfvccwe" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hexo/">hexo</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/学习/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/游记/">游记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wifi-recognition/">wifi recognition</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/人脸识别/">人脸识别</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/关键帧抽取/">关键帧抽取</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/南太行/">南太行</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/土耳其/">土耳其</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/在路上/">在路上</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/新疆/">新疆</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/游记/">游记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/西藏/">西藏</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/语音识别/">语音识别</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/重庆/">重庆</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/wifi-recognition/" style="font-size: 10px;">wifi recognition</a> <a href="/tags/人脸识别/" style="font-size: 10px;">人脸识别</a> <a href="/tags/关键帧抽取/" style="font-size: 10px;">关键帧抽取</a> <a href="/tags/南太行/" style="font-size: 10px;">南太行</a> <a href="/tags/土耳其/" style="font-size: 10px;">土耳其</a> <a href="/tags/在路上/" style="font-size: 10px;">在路上</a> <a href="/tags/新疆/" style="font-size: 10px;">新疆</a> <a href="/tags/深度学习/" style="font-size: 20px;">深度学习</a> <a href="/tags/游记/" style="font-size: 15px;">游记</a> <a href="/tags/西藏/" style="font-size: 10px;">西藏</a> <a href="/tags/语音识别/" style="font-size: 15px;">语音识别</a> <a href="/tags/重庆/" style="font-size: 10px;">重庆</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/08/17/土耳其攻略/">土耳其攻略</a>
          </li>
        
          <li>
            <a href="/2018/08/17/《侣行》/">《侣行》</a>
          </li>
        
          <li>
            <a href="/2018/08/15/西藏攻略/">西藏攻略</a>
          </li>
        
          <li>
            <a href="/2018/08/15/《开车带狗去西藏27天》/">《开车带狗去西藏27天》</a>
          </li>
        
          <li>
            <a href="/2018/07/19/深度压缩-入门/">深度压缩--入门</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>